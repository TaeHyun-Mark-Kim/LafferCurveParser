import pandas as pd
import urllib.request
from bs4 import BeautifulSoup
import re


#Need to install bs4 using the command 'pip install beatifulsoup4'

class Webscraper:

    def __init__(self):

        #Collects urls from a user-provided txt file
        txt_file = input('Please provide the name of txt file with request urls\n')
        with open(txt_file) as f:
            self.url_list = f.readlines()

        #List of initial urls
        self.url_list = [u.strip() for u in self.url_list]
        #Urls containing text files to process
        self.txt_list = []
        #regex templates to parse the data
        self.templates = self.generate_templates()
        #regex templates for titles to separate csv files
        self.title_templates = self.generate_title_templates()
        #Pandas dataframe generated by each text file
        self.data_tables = []

    #For each txt url, create a pandas dataframe
    def txt_to_df(self):
        for url in self.txt_list:
            try:
                self.fn_txt(url)
            except:
                print("Failed to extract table from " + str(url))

    def url_to_csv(self, depth):
        self.scrap_txt_url(depth)
        print('URLs to Process: \n')
        self.print_txt_url()
        sample.txt_to_df()
        #sample.print_tables()
        #sample.df_to_csv_all()

    #Searchs web for txt files with the given depth
    def scrap_txt_url(self, depth):
        self.scrap_txt_urls_helper(self.url_list, 0, depth)

    #Recursive helper function for scrap_txt_url
    def scrap_txt_urls_helper(self, urls, count, limit):
        if count >= limit:
            for url in urls:
                if '.txt' in url or '.TXT' in url:
                    self.txt_list.append(url)
            return
        else:
            new_urls = []
            for url in urls:
                #If the url points to a text file, put it in a list to analyze later
                if '.txt' in url or '.TXT' in url:
                    self.txt_list.append(url)
                #If not, add the links contained within the page for further search
                else:
                    try:
                        URL_eg = urllib.request.urlopen(url)
                        soup = BeautifulSoup(URL_eg)
                        for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
                            #add links as new urls to process
                            new_url = link.get('href')
                            new_urls.append(new_url)
                    except:
                        print("Couldn't connect to a link " + str(url))
                self.scrap_txt_urls_helper(new_urls, count+1, limit)

    def generate_templates(self):
        template_list = []
        """
        To parse the data into separate columns, regex templates need to be
        hard-coded and added to the list
        """
        #regex based on http://www.ksbc.kerala.gov.in/circular/prmtrlisthigh1218.txt
        template1 = ('.*\s(?P<SERIAL_NO>[0-9]+)\s(?P<ITEM_CODE>[0-9][0-9][0-9][0-9][0-9][0-9]).*\s\s'
        '(?P<DESCRIPTION>[A-Z].*[A-Z])\s.*\s(?P<MAX_AMOUNT>[0-9]+[.][0-9][0-9]).*')
        template_list.append(template1)

        #regex based on http://ksbc.kerala.gov.in/reports/may_june_july/RMCI.TXT
        template2 = ('.*(?P<Product_Code>[0-9]{9}|[0-9]{8}[A-Z])\s*(?P<Description>[A-Z].*[A-Z])\s.*\s(?P<UOM>[0-9]+)'
        '\s*(?P<L_Cost>[0-9]+[.][0-9][0-9])\s*(?P<Stock>[0-9]+)\s*(?P<Amount>[0-9]+[.][0-9][0-9]).*')
        template_list.append(template2)

        #User's custom templates...

        return template_list

    def generate_title_templates(self):
        title_template_list = []
        """
        Hard-coded title regex for separating csv files
        """

        #regex based on http://www.ksbc.kerala.gov.in/circular/prmtrlisthigh1218.txt
        template1 = '.*\s(?P<REGION>[A-Z][A-Z][A-Z])\s(?P<TOWN>[A-Z]*)\s.*\s(?P<DATE>[A-Z][A-Z][A-Z]\s[0-9][0-9][0-9][0-9]).*'
        title_template_list.append(template1)

        template2 = '.*\'(?P<CODE>[0-9][0-9][0-9])\s(?P<DISTILLERY>[A-Z].*[A-Z])\s'
        title_template_list.append(template2)

        #User's custom titles for separate csv files...

        return title_template_list

    #Determines whether given textfile's contents matches template
    def match_template(self, txt_content, tmp):
        limit = min(len(txt_content), 50)
        found = False
        i = 0
        #Check first 50 lines to see if there's a match
        while found is False and i < limit:
            match = re.match(tmp, str(txt_content[i]))
            if match:
                found = True
            i +=1
        return found

    def fn_txt(self, url):
        print('Reading ' + url + '...')
        #First, add dataframe with no column separation
        URL_eg = urllib.request.urlopen(url)
        df = pd.read_csv(URL_eg, sep='\t')
        self.data_tables.append(df)

        #If text file's contents match any of the templates,
        #add the column-separated data table
        myfile = urllib.request.urlopen(url)
        data = myfile.readlines()
        template = None
        title_template = None
        #Determine whether there is a data template that matches
        for t in self.templates:
            if self.match_template(data, t):
                print('match!')
                print(t)
                template = t
                break

        #Determine whether there is a title template that matches
        for t in self.title_templates:
            if self.match_template(data, t):
                print('title match!')
                print(t)
                title_template = t
                break

        #Create separate csv files for each title
        if title_template is not None and template is not None:
            dict_list = []
            acc_dict_list = []
            title = ''
            date = ''
            for line in data:
                title_match = re.match(title_template, str(line))
                if title_match:
                    #Check whether the new title is same as one before,
                    #may need to use a hashmap to keep track later on but for now
                    #duplicate titles are contiguous so simpler solution is used
                    #print(line)
                    new_title = ''
                    match_dict = title_match.groupdict()
                    for key in match_dict:
                        new_title += match_dict[key] + ' '
                    #print(new_title)

                    if 'DATE' in match_dict:
                        new_date = match_dict['DATE']

                    if title != new_title:
                        #Create df and csv of the previous title
                        #print(dict_list)
                        #print(title)
                        if len(dict_list) > 0:
                            new_df = pd.DataFrame(dict_list)
                            self.data_tables.append(new_df)
                            self.df_to_csv(new_df, title, True)
                        dict_list = []
                        title = new_title
                        date = new_date
                else:
                    template_match = re.match(template, str(line))
                    if template_match:
                        #Add date parameter to groupdict
                        match_dict = template_match.groupdict()
                        if len(date) > 0:
                            match_dict['DATE'] = date
                        dict_list.append(match_dict)
                        acc_dict_list.append(match_dict)

            #Catch that last csv file!
            print(title)
            new_df = pd.DataFrame(dict_list)
            self.data_tables.append(new_df)
            self.df_to_csv(new_df, title, True)

            #Create accumulated csv files
            new_df = pd.DataFrame(acc_dict_list)
            self.data_tables.append(new_df)
            self.df_to_csv(new_df, url, False)

        #Create comprehensive csv file
        elif template is not None:
            dict_list = []
            for line in data:
                template_match = re.match(template, str(line))
                if template_match:
                    dict_list.append(template_match.groupdict())
            new_df = pd.DataFrame(dict_list)
            self.df_to_csv(new_df, url, False)
            self.data_tables.append(new_df)

    def print_txt_url(self):
        print(self.txt_list)

    def print_tables(self):
        for t in self.data_tables:
            print(t)

    def df_to_csv(self, data_table, title, titled):
        title_final = title
        if not titled:
            title_final = str(title).replace('.txt', '').replace('.pdf', '').replace('.TXT', '')
            title_parsed_list = title_final.rsplit('/', 1)
            title_final = title_parsed_list[len(title_parsed_list) - 1]
        try:
            if len(data_table.columns.values) is 0:
                data_table.to_csv(str(title_final) + '.csv', sep='\t', encoding='utf-8', index=True)
            else:
                print(data_table.columns.values)
                data_table.to_csv(str(title_final) + '.csv', columns=data_table.columns.values, index=True)
        except:
                print('Failed to convert ' + title_final +  ' into csv')


    #Merge tables gathered from different txt files into a single dataframe
    def merge_tables(self):
        result = pd.concat(self.data_tables)
        return result

    #Converts all datatables into csvfiles
    #Deprecated
    def df_to_csv_all(self):
        #Uncomment below block if the user wants to merge all tables from different txt files
        """"
        final_table = self.merge_tables()
        self.data_tables.append(final_table)
        """
        count = 1
        for t in self.data_tables:
            try:
                if len(t.columns.values) is 0:
                    t.to_csv('data' + str(count) + '.csv', sep='\t', encoding='utf-8', index=True)
                else:
                    t.to_csv('data' + str(count) + '.csv', columns=t.columns.values, index=True)
            except:
                print('Failed to convert a dataframe into csv')
            finally:
                count+=1


sample = Webscraper()
sample.url_to_csv(depth=1)
"""
sample.scrap_txt_url(2)
sample.print_txt_url()
sample.txt_to_df()
sample.print_tables()
sample.df_to_csv()
"""
